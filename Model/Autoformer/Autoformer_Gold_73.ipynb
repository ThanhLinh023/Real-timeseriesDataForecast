{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1G3ltBpgwN_ZAdK92bTOvgv_PvlvweJo8","authorship_tag":"ABX9TyMzrmIWCJGPrxaPg+Q52Ko/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aL2FXZZCxymw","outputId":"4201c8d2-4c7e-4bb9-d953-c0230445127a","executionInfo":{"status":"ok","timestamp":1717055076561,"user_tz":-420,"elapsed":9962,"user":{"displayName":"Vương Thanh Linh","userId":"04439363175921131833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/43.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m681.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n"]}]},{"cell_type":"code","execution_count":37,"metadata":{"id":"sPE1DHOWHAsS","executionInfo":{"status":"ok","timestamp":1717064615401,"user_tz":-420,"elapsed":468,"user":{"displayName":"Vương Thanh Linh","userId":"04439363175921131833"}}},"outputs":[],"source":["# Import necessary library\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from torch import nn\n","import torch.nn.functional as F\n","from einops import rearrange,repeat\n","from einops.layers.torch import Rearrange\n","import math\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","source":["# Reimplement Autoformer's classes to build model\n","\n","# Normalize layers based on number of features\n","class LayerNorm(nn.Module):\n","    def __init__(self,nfeat):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(nfeat)\n","\n","    def forward(self,X):\n","        X_norm = self.norm(X)\n","        return X_norm - X_norm.mean(dim = 1).unsqueeze(dim = 1)\n","\n","# Moving average\n","class MovingAvg(nn.Module):\n","    def __init__(self, kernel_size, stride):\n","        super().__init__()\n","        self.kernel_size = kernel_size\n","        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n","\n","    def forward(self, x):\n","        # Padding on the both ends of time series\n","        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n","        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n","        x = torch.cat([front, x, end], dim=1)\n","        x = self.avg(x.permute(0, 2, 1))\n","        x = x.permute(0, 2, 1)\n","        return x\n","\n","# Decomposite the time series\n","class SeriesDecomp(nn.Module):\n","    def __init__(self, kernel_size):\n","        super().__init__()\n","        self.moving_avg = MovingAvg(kernel_size, stride=1)\n","\n","    def forward(self, x):\n","        moving_mean = self.moving_avg(x)\n","        res = x - moving_mean\n","        return res, moving_mean\n","\n","# Auto attention instead of Full attention in Transformer\n","class AutoAttentionLayer(nn.Module):\n","    def __init__(self,nfeat,nhid,nhead, kernel_size = 25, dropout = 0.05):\n","        super().__init__()\n","        self.q = nn.Linear(nfeat,nhid*nhead)\n","        self.k = nn.Linear(nfeat,nhid*nhead)\n","        self.v = nn.Linear(nfeat,nhid*nhead)\n","        self.nhead = nhead\n","        self.out = nn.Linear(nhead*nhid,nfeat)\n","\n","        self.decomp1 = SeriesDecomp(kernel_size)\n","        self.decomp2 = SeriesDecomp(kernel_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.ffn = nn.Sequential(\n","            Rearrange('b l f -> b f l'),\n","            nn.Conv1d(nfeat,2*nfeat,kernel_size = 3, padding=1, bias = False),\n","            nn.Dropout(dropout),\n","            nn.ReLU(),\n","            nn.Conv1d(2*nfeat,nfeat,kernel_size = 3, padding=1, bias = False),\n","            nn.Dropout(dropout),\n","            Rearrange('b f l -> b l f')\n","        )\n","\n","    def forward(self,X,Y=None,Z=None,return_trend = False):\n","        self.topk = int(math.log(X.shape[1])+1)\n","\n","        if Y is None and Z is None:\n","            qkv = (self.q(X),self.k(X),self.k(X))\n","        else:\n","            qkv = (self.q(X),self.k(Y),self.v(Z))\n","\n","        q,k,v = map(lambda x: rearrange(x,'b t (h f)-> b h f t',h = self.nhead),qkv)\n","\n","        Tq,Tk = q.shape[-1],k.shape[-1]\n","        if Tq > Tk:\n","            v = F.pad(v,(0,Tq-Tk),\"constant\",0)\n","            k = F.pad(k,(0,Tq-Tk),\"constant\",0)\n","        else:\n","            v = v[:,:Tq]\n","            k = k[:,:Tq]\n","\n","        # Get time series autocorrelation\n","        q_fft = torch.fft.rfft(q,dim=-1)\n","        k_fft = torch.conj(torch.fft.rfft(k,dim=-1))\n","        lag = torch.fft.irfft(q_fft*k_fft)\n","        # Use lag\n","        corr = lag.mean(dim=(1,2))\n","        weights,delays = torch.topk(corr,self.topk,dim=-1)\n","        delays = torch.topk(weights.mean(dim=0),self.topk,dim=-1)[1]\n","        weights = torch.stack([weights[:,idx] for idx in delays],dim = -1)\n","        weights  = F.softmax(weights,dim=-1)\n","        # Fusion\n","        values = v.clone()\n","        for idx in range(self.topk):\n","            values[:,:,:,idx] = torch.roll(v[:,:,:,idx],-int(delays[idx]),-1) * rearrange(weights[:,idx],\"b ->b 1 1 \")\n","\n","        values = self.out(rearrange(values,'b h f t -> b t (h f)'))\n","\n","        X = X + self.dropout(values)\n","\n","        # Get residual (seasonality)\n","        y, trend1 = self.decomp1(X)\n","        y = self.ffn(y) #ffn\n","        res,trend2 = self.decomp2(X+y)\n","\n","        if not return_trend:\n","            return res\n","        else:\n","            return res,trend1+trend2\n","\n","# Encoder to encode time series input\n","class Encoder(nn.Module):\n","    def __init__(self,nlayers = 2,nfeat = 2048,nhid = 256,nhead = 8, kernel_size = 25,dropout = 0.05):\n","        super().__init__()\n","        self.modulelist = nn.ModuleList([AutoAttentionLayer(nfeat,nhid,nhead,kernel_size,dropout) for i in range(nlayers)])\n","        self.norm = LayerNorm(nfeat)\n","\n","    def forward(self,X):\n","        for m in self.modulelist:\n","            X = m(X)\n","        X = self.norm(X)\n","        return X\n","\n","# Decoder layer to decode and give prediction\n","class DecoderLayer(nn.Module):\n","    def __init__(self,nfeat, nembed = 2048,nhid = 256,nhead = 8, kernel_size = 25,dropout = 0.05):\n","        super().__init__()\n","        self.self_attn = AutoAttentionLayer(nembed,nhid,nhead,kernel_size,dropout)\n","        self.cross_attn = AutoAttentionLayer(nembed,nhid,nhead,kernel_size,dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        self.decomp = SeriesDecomp(kernel_size)\n","        self.projection = nn.Sequential(\n","                Rearrange('b t f-> b f t'),\n","                nn.Conv1d(in_channels=nembed, out_channels=nfeat, kernel_size=3, padding=1,\n","                                        padding_mode='circular', bias=False),\n","                Rearrange('b f t -> b t f'),\n","            )\n","\n","    def forward(self,X,Y):\n","        X = X + self.dropout(self.self_attn(X))\n","        season1,trend1 = self.decomp(X)\n","\n","        seanson2, trend2 = self.cross_attn(season1,Y,Y,return_trend = True)\n","        season = season1 + self.dropout(seanson2)\n","\n","        trend = self.projection(trend1+trend2)\n","        # return X,trend\n","        return season,trend\n","\n","class Decoder(nn.Module):\n","    def __init__(self,nlayers = 1,nfeat = 8 , nembed = 2048,nhid = 256,nhead = 8, kernel_size = 25,dropout = 0.05):\n","        super().__init__()\n","        self.modulelist = nn.ModuleList([DecoderLayer(nfeat,nembed,nhid,nhead,kernel_size,dropout) for i in range(nlayers)])\n","        self.norm = LayerNorm(nfeat)\n","        self.projection = nn.Sequential(\n","                Rearrange('b t f-> b f t'),\n","                nn.Conv1d(in_channels=nembed, out_channels=nfeat, kernel_size=3, padding=1,\n","                                        padding_mode='circular', bias=False),\n","                Rearrange('b f t -> b t f'),\n","            )\n","\n","    def forward(self,X,Y,trend):\n","        for m in self.modulelist:\n","            X,residual_trend = m(X,Y)\n","            trend += residual_trend\n","\n","        X = self.norm(self.projection(X))\n","        return X,trend\n","\n","# Use position embedding to avoid no time stamp input\n","class PosEmbedding(nn.Module):\n","    def __init__(self,nfeat,max_length = 20000):\n","        super().__init__()\n","        position = torch.arange(0, max_length).float().unsqueeze(1)\n","        div_term = (torch.arange(0, nfeat, 2).float() * -(math.log(10000.0) / nfeat)).exp()\n","\n","        self.pe = torch.zeros(max_length, nfeat).float()\n","        self.pe[:, 0::2] = torch.sin(position * div_term)\n","        self.pe[:, 1::2] = torch.cos(position * div_term)\n","        self.pe = self.pe.unsqueeze(0)\n","    def forward(self, x):\n","        return self.pe[:, :x.size(1),:].to(x.device)\n","\n","# Temporal embedding can highly improve the performance\n","class TemporalEmbedding(nn.Module):\n","    def __init__(self, nfeat, max_size = 100, max_stamps = 20):\n","        super(TemporalEmbedding, self).__init__()\n","        self.max_stamps = max_stamps\n","        self.max_size = max_size\n","\n","        self.embed_list = nn.ModuleList()\n","        for i in range(max_stamps):\n","            self.embed_list.append(nn.Embedding(max_size, nfeat))\n","\n","    def forward(self, x):\n","        num_stamps = x.shape[-1]\n","        x = x.long()\n","\n","        embeddings_list = []\n","        for i in range(num_stamps):\n","            embed = self.embed_list[i]\n","            embeddings_list.append(embed(x[:,:,i]))\n","        return sum(embeddings_list)\n","\n","# Autoformer model class\n","class AutoFormer(nn.Module):\n","    def __init__(self,pred_length = None ,encoder_layers=2,decoder_layers=1,nfeat = 8,nembed = 2048,nhid = 256,nhead = 8, kernel_size = 25,dropout = 0.05):\n","        super().__init__()\n","        self.enc_embedding = nn.Sequential(\n","                Rearrange('b t f-> b f t'),\n","                nn.Conv1d(in_channels=nfeat, out_channels=nembed, kernel_size=3, padding=1,\n","                                        padding_mode='circular', bias=False),\n","                Rearrange('b f t -> b t f'),\n","            )\n","        self.dec_embedding = nn.Sequential(\n","                Rearrange('b t f-> b f t'),\n","                nn.Conv1d(in_channels=nfeat, out_channels=nembed, kernel_size=3, padding=1,\n","                                        padding_mode='circular', bias=False),\n","                Rearrange('b f t -> b t f'),\n","            )\n","        self.trend_projection = nn.Linear(nembed,nfeat)\n","        self.seasonal_projection = nn.Linear(nembed,nfeat)\n","\n","        self.encoder = Encoder(encoder_layers,nembed,nhid,nhead,kernel_size,dropout)\n","        self.decoder = Decoder(decoder_layers,nfeat,nembed,nhid,nhead,kernel_size,dropout)\n","\n","        self.decomp = SeriesDecomp(kernel_size)\n","\n","        self.pos_embedding = PosEmbedding(nembed)\n","        self.time_embedding = TemporalEmbedding(nembed)\n","        self.pred_length = pred_length\n","\n","    def forward(self,X_enc,X_stamps = None,y_stamps = None):\n","        T = X_enc.shape[1]\n","        # Decoder input preparation\n","        mean = torch.mean(X_enc, dim=1).unsqueeze(1).repeat(1, self.pred_length, 1)\n","        zeros = torch.zeros_like(mean,device = X_enc.device)\n","\n","        # Decomposition\n","        seasonal_init,trend_init = self.decomp(X_enc)\n","        trend_init = torch.cat([trend_init[:, -self.pred_length:, :], mean], dim=1)\n","        seasonal_init = torch.cat([seasonal_init[:, -self.pred_length:, :], zeros], dim=1)\n","\n","        # Encoder\n","        X_enc = self.enc_embedding(X_enc)\n","        if X_stamps is None:\n","            X_enc += self.pos_embedding(X_enc)\n","        else:\n","            X_enc += self.time_embedding(X_stamps)\n","\n","        enc_out = self.encoder(X_enc)\n","        # Decoder\n","        X_dec = self.dec_embedding(seasonal_init)\n","        if y_stamps is None:\n","            X_dec += self.pos_embedding(X_dec)\n","        else:\n","            X_dec[:,-self.pred_length:,:] = X_dec[:,-self.pred_length:,:] + self.time_embedding(y_stamps)\n","\n","        seasonal_part,trend_part = self.decoder(X_dec,enc_out,trend_init)\n","        X = seasonal_part + trend_part\n","        X = X[:,-self.pred_length:,:]\n","\n","        return X"],"metadata":{"id":"IMdIneIeqywV","executionInfo":{"status":"ok","timestamp":1717064618843,"user_tz":-420,"elapsed":591,"user":{"displayName":"Vương Thanh Linh","userId":"04439363175921131833"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Class to read and process dataset, split parameter is used to split to train and test set\n","class dataset(Dataset):\n","    def __init__(self,input_length = 96,preds_length = 96 ,path = '/content/drive/MyDrive/Third Year/Sem 2/Business Data Analyst/dataset/Gold.csv',\n","                 train = 'train',split = (0.7,0.3)):\n","        super().__init__()\n","        df = pd.read_csv(path)\n","        # Time series feature\n","        df_stamp = pd.DataFrame()\n","        df['Date'] = pd.to_datetime(df['Date'])\n","        df_stamp['Month'] = df['Date'].apply(lambda row:row.month,1)\n","        df_stamp['Weekday'] = df['Date'].apply(lambda row:row.weekday(),1)\n","        df_stamp['Hour'] = df['Date'].apply(lambda row:row.hour,1)\n","        df_stamp['Day'] = df['Date'].apply(lambda row:row.day,1)\n","        df_stamp = df_stamp[['Month','Weekday','Hour','Day']].values\n","\n","        # Scaled data\n","        cols = ['Open','Close','High','Low']\n","        df = df[cols]\n","        self.scale_fn = StandardScaler()\n","        train_length = int((len(df)-input_length-preds_length)*split[0])\n","        test_length = int((len(df)-input_length-preds_length)*split[1])\n","        train_data = df[:train_length].values\n","        self.scale_fn.fit(train_data)\n","        df = self.scale_fn.transform(df.values)\n","\n","        if train == 'train':\n","            self.df = df[:train_length]\n","            self.df_stamp = df_stamp[:train_length]\n","        else:\n","            self.df = df[train_length+test_length:]\n","            self.df_stamp = df_stamp[train_length+test_length:]\n","\n","        self.input_length = input_length\n","        self.preds_length = preds_length\n","        self.train = train\n","\n","        self.df = torch.tensor(self.df,device = 'cuda' if torch.cuda.is_available() else 'cpu').float()\n","        self.df_stamp = torch.tensor(self.df_stamp,device = 'cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","    def __inverse_transform__(self,X):\n","        return self.scale_fn.inverse_transform(X)\n","\n","    def __len__(self):\n","        return len(self.df) - self.input_length - self.preds_length + 1\n","\n","    def __getitem__(self,index):\n","        X = self.df[index:index+self.input_length]\n","        X_stamp = self.df_stamp[index:index+self.input_length]\n","\n","        y = self.df[index+self.input_length:index+self.input_length+self.preds_length]\n","        y_stamp = self.df_stamp[index+self.input_length:index+self.input_length+self.preds_length]\n","\n","        return X,X_stamp,y,y_stamp"],"metadata":{"id":"1_XHcYoglKMf","executionInfo":{"status":"ok","timestamp":1717064623185,"user_tz":-420,"elapsed":352,"user":{"displayName":"Vương Thanh Linh","userId":"04439363175921131833"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Function used to train model\n","def train_per_epoch(dataloader):\n","    model.train()\n","    loss_report = 0.\n","    bar = tqdm(dataloader)\n","    for X,X_stamp,y,y_stamp in bar:\n","        optim.zero_grad()\n","        X = X.float()\n","        y = y.float()\n","        pred = model(X,X_stamp,y_stamp)\n","        loss = loss_fn(y,pred)\n","        loss.backward()\n","        optim.step()\n","        schedule.step()\n","        loss_report += loss.clone().detach().cpu().item()\n","        # bar.set_description(\"Train loss {:.4f} \".format(loss.clone().detach().cpu().item()))\n","    return loss_report/len(dataloader)\n","\n","# Evaluate model after training\n","def valid_per_epoch(dataloader):\n","    model.eval()\n","    loss_report = 0.\n","    bar = tqdm(dataloader)\n","    preds = []\n","    labels = []\n","    for X,X_stamp,y,y_stamp in bar:\n","        with torch.no_grad():\n","            X = X.float()\n","            y = y.float()\n","            pred = model(X,X_stamp,y_stamp)\n","            loss = loss_fn(y,pred)\n","            loss_report += loss.clone().detach().cpu().item()\n","            # bar.set_description(\"Valid loss {:.4f}\".format(loss.clone().detach().cpu().item()))\n","\n","            preds.append(pred.clone().detach().cpu().numpy())\n","            labels.append(y.clone().detach().cpu().numpy())\n","\n","    preds = np.concatenate(preds,axis = 0)\n","    labels = np.concatenate(labels,axis = 0)\n","    loss_report = {\n","        'loss': loss_report/len(dataloader),\n","        'mape': np.abs(((labels-preds)/labels)).mean(),\n","        'mse': ((labels-preds)**2).mean(),\n","        'rmse': math.sqrt(((labels-preds)**2).mean())\n","    }\n","    return loss_report\n","\n","def predict_future(dataloader, device):\n","    model.eval()\n","    y_pred_list = []\n","    y_list = []\n","    with torch.no_grad():\n","        for index,samples in enumerate(dataloader):\n","            x,y_label=samples\n","            x = x.to(device)\n","            y_label = y_label.to(device)\n","            y_pred=model(x)\n","            y_pred_list.extend(y_pred.tolist())\n","            y_list.extend(y_label.tolist())\n","\n","        y_pred_list=np.array(y_pred_list)\n","        y_pred_list=y_pred_list.reshape(y_pred_list.shape[0],1)\n","        y_list=np.array(y_list)\n","        y_list=y_list.reshape(y_list.shape[0],1)\n","    return y_pred_list"],"metadata":{"id":"a03nhAV0zZF6","executionInfo":{"status":"ok","timestamp":1717064626418,"user_tz":-420,"elapsed":467,"user":{"displayName":"Vương Thanh Linh","userId":"04439363175921131833"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["epoch = 15\n","batch_size = 32\n","lr = 1e-4\n","input_length = 24\n","pred_length = 24\n","\n","train_dataset = dataset(input_length,pred_length,train = 'train')\n","test_dataset = dataset(input_length,pred_length,train = 'test')\n","\n","\n","train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,drop_last=False)\n","test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True,drop_last=False)"],"metadata":{"id":"G9qgO51qIKVU","executionInfo":{"status":"ok","timestamp":1717064633271,"user_tz":-420,"elapsed":310,"user":{"displayName":"Vương Thanh Linh","userId":"04439363175921131833"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["model = AutoFormer(pred_length,2,1,nfeat=4,nembed=8,nhid = 512,nhead = 8)\n","loss_fn = torch.nn.MSELoss()\n","optim = torch.optim.Adam(model.parameters(),lr=lr)\n","schedule = torch.optim.lr_scheduler.CosineAnnealingLR(optim,T_max=len(train_dataloader)*epoch,eta_min=1e-7)\n","\n","for e in range(epoch):\n","    print(f'Epoch {e+1}/{epoch}:')\n","    loss_train = train_per_epoch(train_dataloader)\n","    report = valid_per_epoch(test_dataloader)\n","    print('Train loss is {:.5f}'.format(loss_train))\n","    print('Test loss is {:.5f} - MAPE is {:.5f} - MSE is {:.5f} - RMSE is {:.5f}'.format(report['loss'],report['mape'],report['mse'],report['rmse']))\n","    print('-----------------------------------------------------')"],"metadata":{"id":"5Sd3pe2Coh6N","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dd85a7e5-7327-4e1e-b8f8-803d8ae4153c","executionInfo":{"status":"ok","timestamp":1717065618596,"user_tz":-420,"elapsed":983393,"user":{"displayName":"Vương Thanh Linh","userId":"04439363175921131833"}}},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:07<00:00,  1.94s/it]\n","100%|██████████| 1/1 [00:00<00:00, 25.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 2.01741\n","Test loss is 1.13109 - MAPE is 0.36192 - MSE is 1.13109 - RMSE is 1.06353\n","-----------------------------------------------------\n","Epoch 2/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:04<00:00,  1.85s/it]\n","100%|██████████| 1/1 [00:00<00:00, 27.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.72692\n","Test loss is 0.88522 - MAPE is 0.33806 - MSE is 0.88522 - RMSE is 0.94086\n","-----------------------------------------------------\n","Epoch 3/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:07<00:00,  1.94s/it]\n","100%|██████████| 1/1 [00:00<00:00, 29.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.60916\n","Test loss is 0.78367 - MAPE is 0.31240 - MSE is 0.78367 - RMSE is 0.88525\n","-----------------------------------------------------\n","Epoch 4/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:03<00:00,  1.83s/it]\n","100%|██████████| 1/1 [00:00<00:00, 27.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.55081\n","Test loss is 0.71043 - MAPE is 0.29128 - MSE is 0.71043 - RMSE is 0.84287\n","-----------------------------------------------------\n","Epoch 5/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:05<00:00,  1.87s/it]\n","100%|██████████| 1/1 [00:00<00:00, 20.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.50845\n","Test loss is 0.67300 - MAPE is 0.28309 - MSE is 0.67300 - RMSE is 0.82037\n","-----------------------------------------------------\n","Epoch 6/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:03<00:00,  1.83s/it]\n","100%|██████████| 1/1 [00:00<00:00, 22.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.47395\n","Test loss is 0.64636 - MAPE is 0.27973 - MSE is 0.64636 - RMSE is 0.80396\n","-----------------------------------------------------\n","Epoch 7/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:06<00:00,  1.89s/it]\n","100%|██████████| 1/1 [00:00<00:00, 26.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.46192\n","Test loss is 0.65337 - MAPE is 0.28105 - MSE is 0.65337 - RMSE is 0.80831\n","-----------------------------------------------------\n","Epoch 8/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:04<00:00,  1.83s/it]\n","100%|██████████| 1/1 [00:00<00:00, 22.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.42597\n","Test loss is 0.63716 - MAPE is 0.27654 - MSE is 0.63716 - RMSE is 0.79823\n","-----------------------------------------------------\n","Epoch 9/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:04<00:00,  1.85s/it]\n","100%|██████████| 1/1 [00:00<00:00, 17.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.42582\n","Test loss is 0.62287 - MAPE is 0.27097 - MSE is 0.62287 - RMSE is 0.78922\n","-----------------------------------------------------\n","Epoch 10/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:04<00:00,  1.84s/it]\n","100%|██████████| 1/1 [00:00<00:00, 17.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.40836\n","Test loss is 0.62974 - MAPE is 0.27136 - MSE is 0.62974 - RMSE is 0.79356\n","-----------------------------------------------------\n","Epoch 11/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:06<00:00,  1.91s/it]\n","100%|██████████| 1/1 [00:00<00:00, 28.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.39113\n","Test loss is 0.63586 - MAPE is 0.27257 - MSE is 0.63586 - RMSE is 0.79741\n","-----------------------------------------------------\n","Epoch 12/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:04<00:00,  1.84s/it]\n","100%|██████████| 1/1 [00:00<00:00, 26.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.39434\n","Test loss is 0.62056 - MAPE is 0.26721 - MSE is 0.62056 - RMSE is 0.78776\n","-----------------------------------------------------\n","Epoch 13/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:05<00:00,  1.87s/it]\n","100%|██████████| 1/1 [00:00<00:00, 29.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.38446\n","Test loss is 0.61982 - MAPE is 0.26712 - MSE is 0.61982 - RMSE is 0.78728\n","-----------------------------------------------------\n","Epoch 14/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:04<00:00,  1.85s/it]\n","100%|██████████| 1/1 [00:00<00:00, 29.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.38314\n","Test loss is 0.61804 - MAPE is 0.26662 - MSE is 0.61804 - RMSE is 0.78616\n","-----------------------------------------------------\n","Epoch 15/15:\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [01:07<00:00,  1.92s/it]\n","100%|██████████| 1/1 [00:00<00:00, 29.31it/s]"]},{"output_type":"stream","name":"stdout","text":["Train loss is 1.38512\n","Test loss is 0.61801 - MAPE is 0.26664 - MSE is 0.61801 - RMSE is 0.78614\n","-----------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}